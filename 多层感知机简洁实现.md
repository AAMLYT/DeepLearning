# 多层感知机简洁实现

## 一、介绍

​	由于线性模型不能够解决非线性问题，所以我们需要找到一种方法让我们模型输入与输出之间关系不在是线性的。在softmax回归中，我们的输入与输出是线性的，输入特征与权重和偏置计算后得到输出，然后通过经过输出得到每个类别的概率，我们如何改变其线性关系呢？首先想到输入与输出都是不可改变的，那我们只能通过改变计算来解决线性问题了，我们是否可以直接对权重与偏置参数进行处理呢？我觉得是不可行的，通过处理权重与偏置仍然不能改变模型的线性问题。不能通过改变权重与偏置来解决线性问题，那么就可以假设我们的输入层与输出层之间还有一层，有了这层之后我们可以对这个层进行处理。让输入特征经过与第一层参数进行计算后得到的结果通过第二层改变其线性的问题。要改变其线性，就需要通过一些手段来解决。我们可以通过对这一层经过一些选择，让其输入与输出不再是线性关系。这里我们通过让我们计算得到的结果与0比较大小，如果大于0，留下，如果小于0，则变为1，这个就是常用的relu()函数。这样我们通过在我们的单层模型输入与输出间加入一层隐藏层，然后改变隐藏层特征就解决了模型只能处理线性问题的问题了。

单层

![单层](https://zh.d2l.ai/_images/softmaxreg.svg)

多层

![](https://zh.d2l.ai/_images/mlp.svg)

## 二、多层感知机简洁实现

```python
import torch
from torch import nn
from d2l import torch as d2l

#获取数据
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

#定义模型
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

#初始化参数
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
        
net.apply(init_weights)

#损失函数
loss = nn.CrossEntropyLoss(reduction='none')

#优化算法
lr = 0.1
trainer = torch.optim.SGD(net.parameters(), lr=lr)

#模型训练
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## 三、激活函数

- **Sigmoid函数**：输出范围在[0,1]之间，适合输出概率值。但它容易造成梯度消失，且计算相对耗时。
- ![](D:\code\深度学习笔记\多层感知机简洁实现.assets\Sigmoid-1745748235300-4.png)
- **Tanh函数**：输出范围在[-1,1]之间，是零中心的，比Sigmoid函数性能更好，但仍存在梯度消失问题。
- ![](D:\code\深度学习笔记\多层感知机简洁实现.assets\Tanh.png)
- **ReLU函数**：当输入为正时，输出与输入相同，否则输出为0。它解决了梯度消失问题，计算效率高，但存在Dead ReLU问题。
- ![](D:\code\深度学习笔记\多层感知机简洁实现.assets\relu.png)
- **Leaky ReLU函数**：对ReLU的改进，允许负输入有一个非零的梯度，可以解决Dead ReLU问题。
- **PReLU函数**：是Leaky ReLU的参数化版本，其斜率参数是通过学习得到的。
- **ELU函数**：对ReLU的另一种改进，负输入时输出为负值的指数函数，有助于生成接近零的平均激活值。
- **SELU函数**：是自归一化的激活函数，通过调整均值和方差实现内部归一化。
- **Swish函数**：是一种自门控的激活函数，其无界性有助于防止梯度消失问题。
- **Mish函数**：是Swish函数的变体，更平滑，但计算复杂度更高。
- **Softmax函数**：常用于输出层，将输出值映射为概率分布，适用于多分类问题。

