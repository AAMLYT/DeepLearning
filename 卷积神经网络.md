# 卷积神经网络

​	在前面几种模型都是把输入以一维的方式输入模型，这种方式就会导致模型对多维度数据处理不理想。图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。

## 一、卷积层

​	卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。

```python
class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
"""
二维卷积

@Param
	in_channels：输入通道数
	out_channels：输出通道数
	kernel_size：卷积核尺寸
	stride：步幅
	padding：填充
	dilation：
	groups：
	bias：
	padding_mode：
	device：设备
	dtype：类型

"""
```



## 二、池化层

​	通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野就越大。它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。

​	对于给定输入元素，最大汇聚层会输出该窗口内的最大值，平均汇聚层会输出该窗口内的平均值。

```python
class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
"""
对由多个输入平面组成的输入信号应用二维最大池化。

@Param
	kernel_size:最大池化窗口的大小
	stride:窗口的步长。默认值为 kernel_size
	padding:在两侧添加的隐式负无穷填充
	dilation:一个控制窗口中元素步长的参数
	return_indices:如果为 True，则会与输出一起返回最大值的索引。对于后续的 torch.nn.MaxUnpool2d 很有用
	ceil_mode:当为 True 时，将使用 ceil 而不是 floor 计算输出形状

"""

class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)
"""
对由多个输入平面组成的输入信号应用二维平均池化。

@Param
	kernel_size:最大池化窗口的大小
	stride:窗口的步长。默认值为 kernel_size
	padding:在两侧添加的隐式负无穷填充
	ceil_mode:将使用 ceil 而不是 floor 计算输出形状
	count_include_pad:将在平均计算中包含零填充
	divisor_override:如果指定，将用作除数，否则将使用池化区域的大小。

"""
...........
```



## 三、1*1卷积层

### 1、作用

(1)改变特征图大小(升维与降维)

​	由于 1*1 并不会改变 height 和 width，改变的是 channels 这一个通道维度大小，通过核与输入特征图的卷积就可以将原本的数据量进行增加或者减少。

(2)跨通道的特征整合

​	当输入通道不在是1时，通过1*1卷积核与输入特征进行卷积后可以将各个通道的特征进行整合。
