# 前向传播与反向传播

## 一、前向传播

​	神经网络的前向传播就是从输入层开始，按照网络的层次结构，每一层的神经元接收上一层神经元的输出作为自己的输入，经过线性变换（加权求和）和非线性变换（激活函数）后，将处理后的结果传递给下一层神经元。这个过程一直持续，直到输出层产生最终的输出结果。它是神经网络进行预测的主要步骤，数据按照正向的方向在网络中流动。

## 二、反向传播

​	反向传播是一种用于计算神经网络中梯度的有效算法。它是基于链式法则，从输出层开始，反向计算损失函数关于网络中每个参数（权重和偏置）的梯度，以便在训练过程中更新参数，使得模型的预测输出与真实标签之间的损失函数值最小化。然后可以通过使用梯度下降等优化算法，根据反向传播计算得到的梯度来更新网络的参数，使得神经网络能够逐渐学习到输入数据和输出标签之间的复杂关系，从而提高模型的预测准确性。

## 三、梯度爆炸

​	梯度爆炸是在计算神经网络中参数的梯度时，梯度的值变得异常大，这会导致在使用基于梯度的优化算法去更新网络参数时，参数会大幅度变化，进而使得网络难以收敛。

主要产生的原因：

- 深层网络的链式求导法则影响。训练多层模型时，每一层的误差对前面各层参数的梯度是通过层层相乘的方式来传递的。如果每一层的梯度都大于 1，当层数过多时，梯度就会快速增大。
- 参数初始化不合理。

## 四、梯度消失

​	梯度消失指的是在反向传播计算参数梯度时，梯度的值变得极小，近乎趋近于零，使得在利用基于梯度的优化算法更新网络参数时，参数很难得到有效调整，然后影响网络的学习和收敛能力。

主要产生的原因：

- 深层网络的链式求导法则影响。如果每一层的梯度都接近0，当层数过多时，梯度就会快速衰减。
- 参数初始化不合理。

## 五、参数初始化

​	为什么要初始化参数：正确的参数可以帮助我们的模型梯度快速下降，减少模型训练时间。错误的参数会让我们在模型训练过程中梯度下降速度变慢，使模型训练时间大幅度上升。而且不好的参数初始化可能会使得模型出现梯度爆炸与梯度消失的问题

初始化方法：

- 零初始化（Zero Initialization）：将所有的权重和偏置初始化为零。它会导致所有的神经元具有相同的更新，并且会带来梯度消失问题等等。

- 随机初始化（Random Initialization）：将权重和偏置随机地初始化为较小的随机值。常见的随机初始化方法包括从均匀分布或高斯分布中随机采样。可能会带来训练不稳定、对称性和梯度消失或爆炸的问题。

- Xavier初始化（Xavier Initialization）：它是一种针对全连接层的参数初始化方法。它根据前一层和后一层神经元的数量来计算权重的初始范围。Xavier初始化将权重初始化为均匀分布或高斯分布中的较小随机值。它有助于保持梯度的方差在不同层之间大致相等。
